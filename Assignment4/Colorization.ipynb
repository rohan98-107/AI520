{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colorization\n",
    "\n",
    "Authors:\n",
    "\n",
    "- Rohan Rele (rsr132)\n",
    "- Aakash Raman (abr103)\n",
    "- Alex Eng (ame136)\n",
    "- Adarsh Patel (aap237)\n",
    "\n",
    "This project was completed for Professor Wes Cowan's Fall 2019 offering of the CS 520: Intro to Artificial Intelligence course, taught at Rutgers University, New Brunswick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we tackle the problem of colorizing black-and-white photos. That is, given an input image for which every pixel only has one numerical value representing lightness, i.e. ranging from white to black, we seek to output an image for which every pixel has three numerical values corresponding to the Red, Green, and Blue (RGB) color channels. \n",
    "\n",
    "The challenge is that grayscale images contain less information than RGB images, so mapping from the former to the latter will certainly involve some perceptual and numerical loss in conversion. To identify this loss, we start with color images, convert them to grayscale, attempt to colorize them, and then compare the result with the original truth color images. In this way, we seek to solve a supervised machine learning problem wherein we attempt to predict the \"true\" coloring of an image when we know what that \"true\" coloring ought to be.\n",
    "\n",
    "To that end, we build and train a neural network to colorize a black-and-white photo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidePrompt": true
   },
   "source": [
    "## Color spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a **color image** with $n \\times m$ pixel dimensions. We consider its numerical representation as an $n \\times m \\times 3$ tensor, which can be thought of as an $n \\times m$ matrix for which each entry corresponds to one pixel and is a matrix with dimension $3 \\times 1$: one value for each of the R, G, and B channels to \"color\" that pixel.\n",
    "\n",
    "For example:\n",
    "\n",
    "$$I_{rgb} = \\begin{bmatrix} \n",
    "    \\begin{bmatrix} r_{0,0} & g_{0,0} & b_{0,0} \\end{bmatrix} &\n",
    "    \\begin{bmatrix} r_{0,1} & g_{0,1} & b_{0,1} \\end{bmatrix} &\n",
    "    \\dots \n",
    "    \\begin{bmatrix} r_{0,m} & g_{0,m} & b_{0,m} \\end{bmatrix} \\\\\n",
    "    {} & \\ddots & {} \\\\\n",
    "    \\begin{bmatrix} r_{1,0} & g_{1,0} & b_{1,0} \\end{bmatrix} &\n",
    "    \\begin{bmatrix} r_{1,1} & g_{1,1} & b_{1,1} \\end{bmatrix} &\n",
    "    \\dots \n",
    "    \\begin{bmatrix} r_{n,m} & g_{n,m} & b_{n,m} \\end{bmatrix}\n",
    "    \\end{bmatrix}$$\n",
    "    \n",
    "\n",
    "where $r_{i,j}, g_{i,j}, b_{i,j} \\in [0,255]$ each represent the $(i,j)$-th pixel's color along the red, green, and blue channels. The reader is likely familiar with the following two colors in RGB:\n",
    "\n",
    "$$(r=0, g=0, b=0) \\rightarrow \\text{black}$$\n",
    "$$(r=255, g=255, b=255) \\rightarrow \\text{white}$$\n",
    "\n",
    "\n",
    "That being said, a **grayscale image** of the same pixel dimensions can intuitively be thought of as an $n \\times m \\times 1$ tensor, since each pixel can only contain one value for its lightness.\n",
    "\n",
    "For example:\n",
    "\n",
    "$$I_{gray} = \\begin{bmatrix}\n",
    "        p_{0,0} & p_{0,1} & \\dots \\ p_{0,m} \\\\\n",
    "        {} & \\ddots & {} \\\\\n",
    "        p_{n,0} & p_{n,1} & \\dots \\ p_{n,m}\n",
    "        \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "where $p_{i,j} \\in [0,255]$ represents the $(i,j)$-th pixel's lightness. For example, we have:\n",
    "\n",
    "$$(p=0) \\rightarrow \\text{black}$$\n",
    "$$(p=255) \\rightarrow \\text{white}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideOutput": true
   },
   "source": [
    "## Color mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for this problem, we can define our desired color mappings more rigorously than just saying \"go from black and white to color.\"\n",
    "\n",
    "We begin with an image with its true coloring, and map it to its RGB tensor form. Then, our neural network will attempt to predict the color values of the image based only on its grayscale information. Using the language of our color spaces, the neural network will predict a 3-tuple of R, G, and B channel values per pixel based off each pixel's inputted grayscale channel value. This process will be described at length later. Then, the resulting matrix will be parsed and saved as an image.\n",
    "\n",
    "In summary, our image conversion process can be represented as a sequence of functions mapping between the aforementioned color spaces as follows:\n",
    "\n",
    "$$\\text{Image} \\rightarrow I_{rgb} \\rightarrow I_{gray} \\xrightarrow{NN} I^{*}_{rgb} \\rightarrow \\text{Image}^{*}$$\n",
    "\n",
    "where the $LHS$ prior to the neural network are simple image conversions, the middle function is a lengthy composition of neural network operations, and the $RHS$ afterwards are also simple image conversions to recover the predicted colorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data is a volume of multiple color images. To modify the above mappings to work on a volume of multiple images, we consider 4D tensors of dimensions $i, j, k, l$ where $i$ is the index of a given image in the list of input images, $j$ is the dimensionality of pixel information (i.e. 3 for RGB images), $k$ is the length of the image, and $l$ is the width of the image. Then, we may pass this entire 4D tensor through our network to encode the information of all images in the input.\n",
    "\n",
    "We source our data from [this publicly available image dataset,](http://www.vision.caltech.edu/Image_Datasets/Caltech101/) provided by Caltech. It contains thousands of pictures of objects of 101 categories, with around 50 images per category. For our purposes, we source just under 1000 of these images, reserving 761 for training and 221 for testing. Although, due to runtime constraints, we use just 100 of the 761 training examples for training.\n",
    "\n",
    "Some examples of our image data include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/train2/bonsai_0005.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/train2/bonsai_0049.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/train2/bonsai_0061.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/train2/dalmatian_0014.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/train2/dalmatian_0018.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/train2/dalmatian_0028.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/train2/starfish_0002.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/train2/starfish_0014.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/train2/starfish_0080.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/train2/rhino_0005.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/train2/rhino_0030.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/train2/rhino_0032.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "    </tr>\n",
    "        \n",
    "        \n",
    "    \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Given a 4D input tensor $T_{imgs}$, we carry out the $LHS$ of the color mapping sequence described above to pre-process each image in the input volume. The necessary vectorizations and color map conversions are accomplished in Python using the package `PIL` for image file parsing and `numpy` for image tensor operations.\n",
    "\n",
    "The below code imports images from the filenames, resizes them to square images of parameterized length and width $n$, and converts them to 4D input tensors representing color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_to_cmatrices(filenames, n):\n",
    "    rets = np.zeros((len(filenames), 3, n, n))\n",
    "    for f in range(len(filenames)):\n",
    "        img = Image.open('./imgs/' + filenames[f])\n",
    "        img = img.convert('RGB')\n",
    "        img = img.resize((n, n), Image.ANTIALIAS)\n",
    "        tmp = np.array(img)\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                rets[f, 0, i, j] = tmp[i][j][0]\n",
    "                rets[f, 1, i, j] = tmp[i][j][1]\n",
    "                rets[f, 2, i, j] = tmp[i][j][2]\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we convert each image to grayscale using the following provided formula:\n",
    "\n",
    "$$Gray(r, g, b) = 0.21r + 0.72g + 0.07b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_grayscale_cmatrices(cmats):\n",
    "    ret = np.zeros((cmats.shape[0], 1, cmats.shape[2], cmats.shape[3]), dtype=np.uint8)\n",
    "    for m in range(cmats.shape[0]):\n",
    "        cmat = cmats[m, :, :, :]\n",
    "        for i in range(cmat.shape[1]):\n",
    "            for j in range(cmat.shape[2]):\n",
    "                ret[m][0][i][j] = 0.21*cmat[0][i][j] + 0.72*cmat[1][i][j] + 0.07*cmat[2][i][j]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting tensor of grayscale images is passed into our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we wish to predict continuous r, g, and b values, each in the range $[0,255]$, we select a neural network which learns various weights in order to produce those three values per pixel for a given input grayscale pixel.\n",
    "\n",
    "For more details, see `NN.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN architecture & implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple, **fully-connected neural network** with **one hidden layer.** \n",
    "\n",
    "After passing in the input grayscale dataset as `grayscale_dataset` and the true-colored image dataset as `true_imgs`, we extract and define the following dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, _, img_size, img_size = grayscale_dataset.shape\n",
    "input_nodes = img_size**2\n",
    "output_nodes = img_size**2 * 3\n",
    "middle_layer_multiple  = 5\n",
    "nodes_in_middle_layer = output_nodes * middle_layer_multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer consists of $n^2$ nodes, where $n$ is the width and height of the input image (i.e. one input per grayscale pixel). \n",
    "\n",
    "The middle layer number of nodes varied, however it was usually set to a 3-10 times the number of output nodes. `middle_layer_multple` was chosen to be 5 as a result of experimentation with balancing runtime and network performance.\n",
    "\n",
    "We flatten the grayscale image into a $n^2 \\times 1$ vector, and the corresponding truth image into a $n^2 \\times 3$ vector, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = [ grayscale_dataset[i,:,:,:].reshape((input_nodes,1)) for i in range(m)]\n",
    "flattened_true = [true_imgs[i,:,:,:].reshape((output_nodes,1)) for i in range(m)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize all weights to be random uniform between $[-k,k]$ where the value of $k$ was varied experimentally over time. Large $k$ tended to have poor results and lead to too many black pixels in the output, or the output being the same for all test cases. Hence, we decided on randomly initializing the weights in the range $[-0.2, 0.2]$ with approximate mean $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_weight_scale = .2\n",
    "layer_2_weight_scale = .2\n",
    "layer_1_weights = 2*layer_1_weight_scale*np.random.random_sample((nodes_in_middle_layer, input_nodes))-layer_1_weight_scale\n",
    "layer_2_weights = 2*layer_2_weight_scale*np.random.random_sample((output_nodes, nodes_in_middle_layer))-layer_2_weight_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each training run-through of the entire dataset, we push an image $j$ at `flattened[j]` through the network feed-forward using the RELU function characterized by:\n",
    "\n",
    "$$RELU(x) = max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2 = np.maximum(0,np.matmul(layer_1_weights, flattened[j]))\n",
    "output = np.minimum(np.maximum(0,np.matmul(layer_2_weights, layer_2)),255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to the following matrix multiplication and dimension bookkeeping:\n",
    "\n",
    "As `layer_1_weights` has dimension $(3m^2*5 \\times m^2)$, the input image at `flattened[j]` has dimension $(m^2 \\times 1)$, so `layer_2` is of dimension $(3m^2*5 \\times 1)$.\n",
    "\n",
    "Between the middle and output layers we used a modified RELU where everything greater than 255 was set to 255, so as to scale the each output between 0 and 255.\n",
    "\n",
    "As `layer_2_weights` has dimension $(3m^2 \\times 3m^2*5)$, so the output image in matrix representation has dimension $(3m^2 \\times 1)$. This is a flattened RGB image in vector form. Hence, the output layer consists of $3 * n^2$ nodes so as to have an input corresponding to the r, g, and b values for each pixel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining code in our neural network training is accomplished in the **back propagation** code included below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical error: loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following loss function to determine the error of a certain pixel's coloring:\n",
    "\n",
    "$$Loss_t = \\sum_{P_{i,j}} (r^{I'}_{i,j} - r^{I_t}_{i,j})^2 + (g^{I'}_{i,j} - g^{I_t}_{i,j})^2 + (b^{I'}_{i,j} - b^{I_t}_{i,j})^2$$\n",
    "\n",
    "for pixel $P_{i,j}$ where $r^{I'}_{i,j}, g^{I'}_{i,j}$, and $b^{I'}_{i,j}$ are this pixel's true coloring, and $r^{I_t}_{i,j}, g^{I_t}_{i,j}$, and $b^{I_t}_{i,j}$ are this pixel's coloring in the current state.\n",
    "\n",
    "The motivation for our chosen loss function was to mimic the **sum of the squared Euclidean distances** between predicted and real r, g, and b values per pixel, for a given image.\n",
    "\n",
    "Given the matrix representations of our images, loss is easily calculated at any time step using `numpy` subtraction and multiplication operations. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = output - flattened_true[j]\n",
    "error = np.matmul(np.transpose(diffs),diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider perceptual error first in terms of how the output image characterizes the shape of the features in the original image, and second in terms of how the output image's colors correspond to the colorization of the original image. We also consider good performance in terms of shape preservation to be a prerequisite to good performance in terms of the colorization. A good colorized image will have both the features and colors of the truth image. A bad colorized image will have neither."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is the integral part of any neural network architecture that minimizes the cost function in order to learn the weights which eventually give the network its predictive capabilities. We begin by outlining the update equation for the weights. Letting $\\textbf{W}$ be *all* the weights in the model and $\\alpha$ be the learning rate, we have:   \n",
    "\n",
    "$$\\textbf{W}_{new} = \\textbf{W}_{old} - \\alpha*\\nabla_{\\textbf{W}_{old}}L$$\n",
    "\n",
    "The difficult term to compute, $\\nabla_{\\textbf{W}_{old}}L$, is nothing more than a vector of the the following derivatives where $i$ is the starting node and $j$ is the ending node (recall a weight can be visualized as an edge that bridges two nodes in a network) and $t$ is the index of the layer number: \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{i,j}^{t-1}}$$ \n",
    "\n",
    "The chain rule says: \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{i,j}^{t-1}} = \\frac{\\partial L}{\\partial out_{j}^{t}}\\frac{\\partial out_{j}^t}{\\partial w_{i,j}^{t-1}}$$\n",
    "\n",
    "where $out_j^t$ is the $jth$ output in layer $t$. Its relationship with the previous output layers gives us, with activation function $\\sigma$ and $\\underline{w}^{t-1}(j)$ being all the weights in layer $t-1$ that map to node $j$: \n",
    "\n",
    "$$\\frac{\\partial out_{j}^t}{\\partial w_{i,j}^{t-1}} = \\sigma'(\\underline{w}^{t-1}(j) \\cdot \\underline{out}^{t-1})out_i^{t-1}$$ \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial out_{j}^{t}} = \\sum_k \\frac{\\partial L}{\\partial out_{k}^{t+1}}\\sigma'(\\underline{w}^{t-1}(k) \\cdot \\underline{out}^{t})w_{j,k}^t $$\n",
    "\n",
    "which finally allows us to define an update equation in an algorithmic manner. \n",
    "\n",
    "\n",
    "At $t=K$, where K is the last layer of the network, define: \n",
    "\n",
    "$$\\Delta_j^K = \\frac{\\partial L}{\\partial out_{j}^{K}}$$\n",
    "\n",
    "$\\forall t < K$\n",
    "\n",
    "$$\\Delta_j^t = \\frac{\\partial L}{\\partial out_{j}^{t}}$$\n",
    "\n",
    "Thus, we can update any weight via the following formula: \n",
    "\n",
    "$$w_{i,j}^{t} := w_{i,j}^t - \\alpha*\\Delta_j^{t+1}*\\sigma'(\\underline{w}^{t-1}(k) \\cdot \\underline{out}^{t})w_{j,k}^t $$\n",
    "\n",
    "In our network architecture, we tried various activation functions, all of which would obviously change the update definition in our backpropogation code. Below are the three activation functions that we tried and their derivatives:\n",
    "\n",
    "1. ReLU\n",
    "$$\\sigma(x) = \\begin{array}{cc}\n",
    "  \\{ & \n",
    "    \\begin{array}{cc}\n",
    "      x & x > 0 \\\\\n",
    "      0 & x \\leq 0\n",
    "    \\end{array}\n",
    "\\end{array} \\\\ \\sigma'(x) = \\begin{array}{cc}\n",
    "  \\{ & \n",
    "    \\begin{array}{cc}\n",
    "      1 & x > 0 \\\\\n",
    "      0 & x \\leq 0\n",
    "    \\end{array}\n",
    "\\end{array}$$\n",
    "\n",
    "For simplicity, we assumed that the derivative of ReLU = 1 everywhere.\n",
    "\n",
    "2. Sigmoid \n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}} \\\\ \\sigma'(x) = \\frac{\\sigma(x) -1}{\\sigma(x)}$$\n",
    "\n",
    "3. Hyperbolic Tangent\n",
    "$$\\sigma(x) = tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\\\ \\sigma'(x) = 1 - \\sigma(x)^2$$\n",
    "\n",
    "For our simple one-layer neural network attempt with just RELU, the backpropagation algorithm was executed by the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize the gradients for Layer 1 (to go to Layer 2) and for Layer 2 (to go to output layer) as matrices with zeroes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_gradients = np.zeros((output_nodes, nodes_in_middle_layer))\n",
    "layer_1_gradients = np.zeros((nodes_in_middle_layer, input_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we calculated the derivative of the loss with respect to the output  as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_deltas  = 2*(output - flattened_true[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and similarly for the derivative of the loss with respect to the second layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_deltas = np.matmul(np.transpose(layer_2_weights),output_deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this together, we updated the gradients and weights for layers 1 and 2 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_gradients += np.matmul(output_deltas, np.transpose(layer_2))\n",
    "layer_1_gradients += np.matmul(layer_2_deltas,np.transpose(flattened[j]))\n",
    "\n",
    "layer_2_weights -= np.minimum(np.maximum(-.01*batch_size*layer_2_weight_scale,\\\n",
    "                        learning_rate * layer_2_gradients*layer_2_weight_scale),.01*batch_size*layer_2_weight_scale)\n",
    "layer_1_weights -= np.minimum(np.maximum(-.01*batch_size*layer_1_weight_scale,\\\n",
    "                        learning_rate**2 * layer_1_gradients*layer_1_weight_scale),.01*batch_size*layer_1_weight_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient update takes into account the gradient at the previous layer matrix-multiplied with the results at the current layer.\n",
    "\n",
    "Note that our weight update is not simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_weights -= learning_rate * layer_2_gradients\n",
    "layer_1_weights -= learning_rate * layer_1_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because with some experimentation, the above scaling we performed led to better results and fewer instances of pixels zero-ing out to black in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time constraints, we did not thoroughly test regularization, which would have accounted for and helped to mediate the possibilty of overfitting.\n",
    "\n",
    "The code change would have simply been in our weight updates, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_weights -= lamda * layer_2_weights + np.minimum(np.maximum(-.01*batch_size*layer_2_weight_scale,learning_rate * layer_2_gradients*layer_2_weight_scale/(i+1)**3),.01*batch_size*layer_2_weight_scale)\n",
    "layer_1_weights -= lamda * layer_1_weights + np.minimum(np.maximum(-.01*batch_size*layer_1_weight_scale,learning_rate**2 * layer_1_gradients*layer_1_weight_scale/(i+1)**3),.01*batch_size*layer_1_weight_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the variable `lamda` corresponds to the $\\lambda$ regularization parameter which would help control the weight updates to prevent overfitting to the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-layer fully-connected neural network described above was most successful out of multiple architectures we attempted (described at the report's end).\n",
    "\n",
    "We trained our model on 100 $16 \\times 16$ images with 300 iterations through the entire dataset and a batch size of $n=50$. Our test set consists of 221 $16 \\times 16$ images.\n",
    "\n",
    "We observed the following decline of loss over time:\n",
    "\n",
    "![Loss Graphs](./imgs/report/loss.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the aggregate training and testing losses descending overall, with significant fluctuations as large as $0.3-0.4 \\times 1 \\times 10^7$. We see that with 300 iterations, the testing loss seems to taper off at around $1 \\times 10^7$ after iteration 250, while the training loss continues to descend. This suggests that after 250-300 iterations, we may begin to worry about overfitting to the training data. \n",
    "\n",
    "But, with just 300 iterations, we should not suspect overfitting just yet, especially with larger concerns over the visuals of the outputs below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a few examples of our colorizations, where the left image is the truth image, the middle is the scaled and grayscale input image, and the right is the final colorized output image:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/05_truth.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/05_gray.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>        \n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/05_color.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "   </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/01_truth.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/01_gray.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>        \n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/01_color.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "   </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/02_truth.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/02_gray.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>        \n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/02_color.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/03_truth.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/03_gray.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>        \n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/03_color.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/04_truth.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/04_gray.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>        \n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/04_color.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "   </tr>\n",
    "       <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/06_truth.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/06_gray.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>        \n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/06_color.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "   </tr>\n",
    "    \n",
    "</table>\n",
    "\n",
    "To see all of our test results, see `./imgs/16test2` for the grayscale and colorized versions, and `./imgs/test2` for the corresponding truth images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, by our metrics and common sense perception, the colorizations are not good.\n",
    "\n",
    "The above six examples show a spectrum of various common colorization errors, which we saw frequently-occuring in our trials. These include:\n",
    "\n",
    "1. **Total lack of feature encoding:** input images' feature shapes were lost in an output image which looks like each pixel was basically colored at random. That is, we obtain a sea of randomly colorized pixels not corresponding to reality.\n",
    "\n",
    "2. **Resolution of feature encoding to stripes:** all of the image's features were encoded as vertical colored stripes running down the image. It is similar to 1., except with some kind of order, although it also does not correspond to reality.\n",
    "\n",
    "3. **Varying levels of unwarranted blackness:** images encoded had significant clusters of black pixels, regardless of how dark the input image was.\n",
    "\n",
    "\n",
    "### Fuzzy features\n",
    "\n",
    "One can immediately see that the produced output images do a very poor job at preserving the initial shapes of the features in the original images. \n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/shape_truth.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/shape_gray.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>        \n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/shape_color.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "   </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/badshape_truth.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/badshape_gray.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>        \n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/badshape_color.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "   </tr>\n",
    "    \n",
    "</table>\n",
    "\n",
    "\n",
    "In some cases, the colorization did capture some of the original image's shape. In the first example above, the bonsai tree outline with approximately three main features - the two triangular-shaped heads and the plant base - is represented roughly in the colorized version as three approximately triangular clusters of colorized pixels. The blackness of the background in comparison to those features is preserved. On the other hand, the colors within these three clusters are totally nonsensical, as the original image had a mostly red color palette.\n",
    "\n",
    "In worse cases, the colorization not only failed to capture original input feature shapes, but it also produced new shapes (i.e. not black pixels) that did not correspond to reality. In the second example, the bonsai tree comprises of one major feature appears like a concave arc curving right on a white background. However, the output image has what appears to be three black pixel clusters (in a smiley-face shape) on a vertically-striped color background. This ultimately looks nothing like the original input.\n",
    "\n",
    "\n",
    "### Black pits of despair\n",
    "\n",
    "One can also see black pixels riddled throughout the predicted images. For example:\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/black_truth.jpg' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/black_gray.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>        \n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src='./imgs/report/black_color.png' alt='missing' />\n",
    "            </figure>\n",
    "        </td>\n",
    "   </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Where did the panda go? The colorization attempts to capture some rough shape, but the rest of the image is effectively \"blacked out.\"\n",
    "\n",
    "We suspect that this is a result of pixels being zeroed out by the training algorithm. We observed that the deltas and gradients calculated in our backpropagation matrix operations often got zeroed out throughout training, by doing simple checks using `np.all() ==  0`. This could be the result of mathematical errors in our matrix operations or the calculation of the derivatives of the loss functions. Alternatively, this may be the result of weights being initialized too close to zero such that subsequent weight updates effectively zero out all the weights. We troubleshooted either of these cases by changing up our mathematical operations or tweaking the weight initialization ranges, but to no avail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our other attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted the following alternative neural network architectures:\n",
    "\n",
    "1. Fully-connected neural network with two hidden layers, using RELU activation functions. See `NN_extra_layer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2 = np.maximum(0,np.matmul(layer_1_weights, flattened[j]))\n",
    "layer_3 = np.maximum(0,np.matmul(layer_2_weights, layer_2))\n",
    "output = np.minimum(np.maximum(0,np.matmul(layer_3_weights, layer_3)),0)\n",
    "output_deltas  = 2*((output- flattened_true[j]))\n",
    "\n",
    "layer_3_deltas = np.matmul(np.transpose(layer_3_weights),output_deltas)\n",
    "layer_2_deltas = np.matmul(np.transpose(layer_2_weights),layer_3_deltas)\n",
    "\n",
    "layer_3_gradients += np.matmul(output_deltas, np.transpose(layer_3))\n",
    "layer_2_gradients += np.matmul(layer_3_deltas, np.transpose(layer_2))\n",
    "layer_1_gradients += np.matmul(layer_2_deltas,np.transpose(flattened[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fully-connected neural network with one hidden layer, using sigmoid activation function for the output step and RELU for inner layer. See `NN_sigmoid.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid for output step\n",
    "output = 255 * (sigmoid(np.matmul(layer_2_weights, layer_2)))\n",
    "\n",
    "output_deltas  = 2*((output- flattened_true[j]))\n",
    "\n",
    "# derivative of sigmoid = sigmoid * (1 - sigmoid)\n",
    "intermediate = output_deltas * ((output * (1 - output))/255)\n",
    "\n",
    "layer_2_deltas = np.matmul(np.transpose(layer_2_weights), intermediate)\n",
    "\n",
    "layer_2_gradients += np.matmul(intermediate, np.transpose(layer_2))\n",
    "layer_1_gradients += np.matmul(layer_2_deltas, (d_sigmoid(np.transpose(flattened[j]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fully-connected neural network with two hidden layers, using sigmoid activation functions for all layers. See `NN_sigmoid_extra_layer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2 = sigmoid(np.matmul(layer_1_weights, flattened[j]))\n",
    "layer_3 = sigmoid(np.matmul(layer_2_weights, layer_2))\n",
    "\n",
    "output = 255 * sigmoid(np.matmul(layer_3_weights, layer_3))\n",
    "\n",
    "output_deltas  = 2*((output- flattened_true[j]))\n",
    "\n",
    "# derivative of sigmoid = sigmoid * (1 - sigmoid)\n",
    "layer_3_intermediate = output_deltas * ((output * (1 - output))/255)\n",
    "layer_3_deltas = np.matmul(np.transpose(layer_3_weights),layer_3_intermediate)\n",
    "\n",
    "layer_2_intermediate = layer_3_deltas * (layer_3 * (1 - layer_3))\n",
    "layer_2_deltas = np.matmul(np.transpose(layer_2_weights), layer_2_intermediate)\n",
    "\n",
    "layer_1_intermediate = layer_2_deltas * (layer_2 * (1 - layer_2))\n",
    "\n",
    "layer_3_gradients += np.matmul(layer_3_intermediate, np.transpose(layer_3))\n",
    "layer_2_gradients += np.matmul(layer_2_intermediate, np.transpose(layer_2))\n",
    "layer_1_gradients += np.matmul(layer_1_intermediate, np.transpose(flattened[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fully-connected neural network with two hidden layers, using hyperbolic tangent activation function. See `NN_tanh_extra_layer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2 = np.tanh(np.matmul(layer_1_weights, flattened[j]))\n",
    "layer_3 = np.tanh(np.matmul(layer_2_weights, layer_2))\n",
    "\n",
    "# scale tanh from [-1,1] to [0,255]\n",
    "output = (np.tanh((np.matmul(layer_3_weights, layer_3))) + 1) * (255/2)\n",
    "\n",
    "output_deltas  = 2*((output- flattened_true[j]))\n",
    "\n",
    "# derivative of tanh(x) = 1 - tanh^2(x)\n",
    "layer_3_intermediate = output_deltas * (1-((output * (2/255))-1)**2)\n",
    "layer_3_deltas = np.matmul(np.transpose(layer_3_weights),layer_3_intermediate)\n",
    "\n",
    "layer_2_intermediate = layer_3_deltas * (1-(layer_3)**2)\n",
    "layer_2_deltas = np.matmul(np.transpose(layer_2_weights), layer_2_intermediate)\n",
    "\n",
    "layer_1_intermediate = layer_2_deltas * (1-(layer_2)**2)\n",
    "\n",
    "layer_3_gradients += np.matmul(layer_3_intermediate, np.transpose(layer_3))\n",
    "layer_2_gradients += np.matmul(layer_2_intermediate, np.transpose(layer_2))\n",
    "layer_1_gradients += np.matmul(layer_1_intermediate, np.transpose(flattened[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, none seemed to produce loss functions which appropriately descending over subsequent training iterations. We observed the following results for each of the four above architectures while also toying with our learning rate $\\alpha$, etc.\n",
    "\n",
    "1. Loss stayed constant\n",
    "2. Loss did not descend consistently\n",
    "3. Loss did not descend consistently\n",
    "4. Loss did not descend significantly\n",
    "\n",
    "Consequently, the resulting images were both visually and mathematically nonsensical. These alternative architectures may have failed because the mathematical definitions they were predicated upon were invalid, our learning rate $\\alpha$ was inappropriate, or because the training set was too small. We suspect that for the sigmoid functions, the former case (mathematical imprecision) is most likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recognize that a better colorization neural network should accomplish any or all of the following enhancements:\n",
    "\n",
    "1. **More hidden layers** to detect/predict more features of images\n",
    "\n",
    "This is extremely crucial to the success of the neural network, but at the same time, it would have made runtimes infeasibly large.\n",
    "\n",
    "2. **More appropriate activation functions** given the task at hand\n",
    "\n",
    "RELU is relatively easy to code and efficient for the machine to execute, but at the same time, we suspect that it is responsible for a majority of images \"blacking out.\" That is, the RELU function may have been zeroing out negative pixel values instead of scaling them or otherwise operating on them to preserve that data.\n",
    "\n",
    "While the sigmoid and hyperbolic functions were less efficient in terms of runtime, they seemed to produce images which at least had some sort of shape beyond random scattering of pixels and colors.\n",
    "\n",
    "In the future, we would want to see which functions are appropriate for which steps, and analyze why.\n",
    "\n",
    "3. **Higher-resolution training and test datasets** of images\n",
    "\n",
    "One key consideration looming over our training process is the fact that our training images are quite small at $16 \\times 16$ pixels. When scaled that small, our training images lost resolution. This might have made it more difficult for our network to learn pixel boundaries of shapes in the images, therefore making the predicted outputs less coherent and representative of reality. Ideally, a successful colorizer should be able to act on images of at least $64 \\times 64$ dimension, where shapes are allotted more pixels to define their patterns.\n",
    "\n",
    "4. **Larger training datasets**\n",
    "\n",
    "Although we sourced approximately a thousand images, we only used 100 for training due to runtime constraints. Obviously, we would have wanted a much more substantial training dataset. It would have been preferable to train on the full 1000-sized dataset, and test on only 100-200 images never before seen by the algorithm. But this training would have taken too long on our machines and for our purposes.\n",
    "\n",
    "5. Tweaking the **learning rate $\\alpha$**\n",
    "\n",
    "We experimented with various $\\alpha$ as necessary to get our training execution to finish in a reasonable time, but more extensive experimentation should be done to observe how $\\alpha$ can change the gradient descent towards avoiding local minima and heading towards true global minima.\n",
    "\n",
    "6. Implementing **regularization parameter $\\lambda$**\n",
    "\n",
    "As described earlier, we would have wanted to implement regularization as a way to protect against overfitting. Given that we were chiefly concerned with diagnosing the more pertinent issues of our output images being nonsensical, we did not prioritize overfitting, but this is ideally a must-have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
