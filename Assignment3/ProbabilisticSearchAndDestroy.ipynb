{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Search and Destroy\n",
    "\n",
    "Authors:\n",
    "\n",
    "- Rohan Rele (rsr132)\n",
    "- Aakash Raman (abr103)\n",
    "- Alex Eng (ame136)\n",
    "- Adarsh Patel (aap237)\n",
    "\n",
    "This project was completed for Professor Wes Cowan's Fall 2019 offering of the CS 520: Intro to Artificial Intelligence course, taught at Rutgers University, New Brunswick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we consider a two-dimensional map of cells in which one cell is randomly designated as the target. The location of the target is not known to any solving agent. Therefore, the problem is to devise an agent which can effectively query the landscape of cells, contribute towards its knowledge base based on observations, and ultimately find the target in the **minimal number of queries.**\n",
    "\n",
    "The knowledge base itself will contain probabilistic knowledge, i.e. \n",
    "\n",
    "$$\\text{Belief}[\\text{Cell}_i] = P(\\text{Target in Cell}_i|  \\text{Observations through time } t)$$ \n",
    "\n",
    "For every cell, this is the probability that a given cell contains the target given the existing knowledge base. Initially, as the agent has no prior knowledge about the map, the belief for each cell is $\\frac{1}{dim^2}$.\n",
    "\n",
    "Each cell also contains a terrain type which corresponds to the probability that a query will return a false negative, i.e.\n",
    "\n",
    "$$P(\\text{Target not found in Cell}_i | \\text{Target is in Cell}_i)$$\n",
    "\n",
    "which is $0.1$ for **flat** terrain cells, $0.3$ for **hilly** terrain cells, $0.7$ for **forested** terrain cells, and $0.9$ for cells whose terrain is a maze of **caves.**\n",
    "\n",
    "We assume that for any given map, each cell is assigned the flat terrain type with probability $0.2$, the hilly terrain type with probability $0.3$, the forested terrain type with probability $0.3$, and the caves terrain type with probability $0.2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landscape\n",
    "\n",
    "We implement the landscape as a class, which has the following fields:\n",
    "\n",
    "- `dim` (int): the dimension of the $dim$ by $dim$ map\n",
    "- `landscape` (2D list) of `landCell` objects, each of which tracks:\n",
    "    - `target` (int): `PRESENT = 1` if this cell is the target, or `ABSENT = 0` otherwise\n",
    "    - `terrain` (int): `FLAT = 0.1` if this cell has flat terrain, `HILLY = 0.3`, `FOREST = 0.7`, or `MAZE = 0.9`, etc.\n",
    "- `target_x` (int): the x-coordinate of the target cell\n",
    "- `target_y` (int): the y-coordinate of the target cell\n",
    "\n",
    "A landscape is initialized with non-target cells that are assigned terrain types based on the probabilities previously described. It then randomly selects one cell to be the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class landscape:\n",
    "\n",
    "    dim = 0\n",
    "    landscape = [[]]\n",
    "    target_x = 0\n",
    "    target_y = 0\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.landscape = [[landCell() for _ in range(self.dim)] for _ in range(self.dim)]\n",
    "\n",
    "        target_x = random.randint(0, dim - 1)\n",
    "        target_y = random.randint(0, dim - 1)\n",
    "        self.landscape[target_x][target_y].target = PRESENT\n",
    "        self.target_x = target_x\n",
    "        self.target_y = target_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more implementation details, see `Landscape.py`.\n",
    "\n",
    "The `landCell` object is also defined in a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class landCell:\n",
    "\n",
    "    def __init__(self):\n",
    "        x = random.randint(1, 100)\n",
    "        self.target = ABSENT\n",
    "        if x <= 20:\n",
    "            self.terrain = FLAT\n",
    "        elif 20 < x <= 50:\n",
    "            self.terrain = HILLY\n",
    "        elif 50 < x <= 80:\n",
    "            self.terrain = FOREST\n",
    "        else:\n",
    "            self.terrain = MAZE\n",
    "\n",
    "    def getTerrain(self):\n",
    "        return self.terrain\n",
    "\n",
    "    def isTarget(self):\n",
    "        return (self.target==PRESENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more implementation details, see `Cell.py`.\n",
    "\n",
    "For example, an initialized $dim = 50$ landscape may look like this:\n",
    "\n",
    "![Blank Landscape](./imgs/landscape_blankTest.png)\n",
    "\n",
    "where the target is located at (40, 48)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement the agent as a class, which has the following fields:\n",
    "\n",
    "- `knowledge` (2D list) of `agentCell` objects, each of which tracks:\n",
    "    - `belief` (float): the probability that a given cell contains the target, as described above; initially $1/{dim}^2$\n",
    "    - `status` (boolean): either `VISITED = True` or `UNVISITED = False` depending on whether or not the cell has been queried previously; initially `False`\n",
    "- `rule` (int): either 1 or 2, corresponding to the two probability rules described below\n",
    "- `num_actions` (int): the number of actions, whether queries or movements (in the later case of a movement-restricted agent), executed so far; initially 0\n",
    "- `i` (int): x-coordinate of first cell to query; if -1, then select one at random\n",
    "- `j` (int): y-coordinate of first cell to query; if -1, then select one at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    num_actions = 0\n",
    "\n",
    "    def __init__(self, landscape, rule):\n",
    "        self.ls = landscape\n",
    "        d = self.ls.dim\n",
    "\n",
    "        if rule == 1 or rule == 2:\n",
    "            self.rule = rule\n",
    "        else:\n",
    "            print(\"Invalid rule, set to 1 by default\")\n",
    "            self.rule = 1\n",
    "\n",
    "        self.knowledge = [[agentCell() for j in range(d)] for i in range(d)]\n",
    "        for i in range(d):\n",
    "            for j in range(d):\n",
    "                self.knowledge[i][j].setBelief(1/(d**2))\n",
    "        \n",
    "        self.i = random.randint(0, self.ls.dim-1) if start_i == -1 else start_i\n",
    "        self.j = random.randint(0, self.ls.dim-1) if start_j == -1 else start_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more implementation details, see `Agent.py`.\n",
    "\n",
    "The `agentCell` object is also defined in a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agentCell:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.belief = 0\n",
    "        self.status = UNVISITED\n",
    "\n",
    "    def getBelief(self):\n",
    "        return self.belief\n",
    "\n",
    "    def getStatus(self):\n",
    "        return self.status\n",
    "\n",
    "    def setBelief(self,belief):\n",
    "        self.belief = belief\n",
    "\n",
    "    def setStatus(self,status):\n",
    "        self.status = status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more implementation details, see `Cell.py`.\n",
    "\n",
    "For example, an initialized agent knowledge base with $dim = 50$ landscape may look like this:\n",
    "\n",
    "![Blank Beliefs](./imgs/belief_blankTest.png)\n",
    "\n",
    "where each cell has initial belief $\\frac{1}{50^2} = 0.0004$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `agent` class also has the following methods (non-exhaustive list):\n",
    "\n",
    "- `searchCell(cell)` (boolean): query a `landCell` object. If it is not the target, return `False`. If it is the target, then only return `True` with probability $p = 1 - P(\\text{false negative})$, where the false negative probability depends on that cell's terrain type as described previously. Otherwise, return `False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchCell(self,cell):\n",
    "    if not cell.isTarget():\n",
    "        return False\n",
    "    else:\n",
    "        p = 1 - cell.getTerrain()\n",
    "        if random.uniform(0, 1) < p:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `getVisited()` (list): return a list of all (x,y) coordinates which the agent has already queried at a given point in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVisited(self):\n",
    "    n = self.ls.dim\n",
    "    coords = []\n",
    "    for x in range(n):\n",
    "        for y in range(n):\n",
    "            if self.knowledge[x][y].getStatus():\n",
    "                coords.append((x,y))\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the belief state\n",
    "\n",
    "We require a method to update the belief state given the results of a query. There are two cases: \n",
    "\n",
    "1. A query of a cell found the target, in which case the belief for this cell is set to 1, and the beliefs for all other cells are set to 0.\n",
    "\n",
    "2. A query of a cell did not find the target, in which case the belief for this cell must be adjusted considering the probability that the query returned a false negative.\n",
    "\n",
    "The latter case considers the probability \n",
    "\n",
    "$$P(\\text{Target in Cell}_i | \\text{Observations}_t \\land \\text{Failure in Cell}_j)$$\n",
    "\n",
    "and relies on the probabilistic knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $H := \\{\\text{Target in Cell}_i\\}$ and $E := \\{\\text{Target not found if we queried every cell}\\}$. \n",
    "\n",
    "Then we want $P(H|E)$, or the probability that the target is in a cell given we have not found the target in any of our queries. We would like to compute this upon a failed query of a cell and accept this quantity as that cell's new belief.\n",
    "\n",
    "By **Bayes' theorem,** this quantity is:\n",
    "\n",
    "$$P(H|E) = \\frac{P(E|H)P(H)}{P(E)}$$\n",
    "\n",
    "Observe that $P(E|H)$ is the probability that the target is not found given the target is in the cell, which is exactly the false negative probability described above per terrain type. And $P(H)$ is exactly the agent's belief for that cell in the previous time step.\n",
    "\n",
    "One can see that observing a failed query for a given cell will decrease our belief that this cell contains the target, but this decrease is scaled by the possibility of false negatives.\n",
    "\n",
    "$P(E)$ is calculated with the following function: \n",
    "\n",
    "$$P(E) = \\sum_{i \\text{ visited}} P(\\text{Target not found in Cell}_i | \\text{Target is in Cell}_i) + \\sum_{j \\text{ unvisited}} P(\\text{Target not in Cell}_j)$$\n",
    "\n",
    "That is, it is the probability that some queried cell was a false negative and that the unqueried cells do not contain the target. In this situation, querying all remaining cells would not lead to us finding the target.\n",
    "\n",
    "Finally, once a queried cell is updated, we must update the rest of the knowledge base. \n",
    "\n",
    "Let $R_i = |{\\text{new belief of Cell}_i} - {\\text{old belief of Cell}_i}|$, or the difference between the new and old beliefs of the queried cell.\n",
    "\n",
    "Then for all remaining cells $j$, use the following update formula: \n",
    "\n",
    "$$\\text{Belief}^{t+1}_j = \\text{Belief}^t_j + \\frac{\\text{Belief}^t_j * R_i}{1 - R_i}$$\n",
    "\n",
    "This, in a sense, scales the previous belief by how much our query impacted the belief of the queried cell. One can see how failed queries will increase the beliefs of all other cells per iteration, although this increase may be marginal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Based on the intuition above, the implementation of a belief update is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateBelief(self,x,y):\n",
    "    #P(H|E) = P(E|H)P(H)/P(E)\n",
    "    #H: Target in cell\n",
    "    #E: Target not found\n",
    "    curr_belief = self.knowledge[x][y].getBelief()\n",
    "    num = self.ls.landscape[x][y].getTerrain()*curr_belief\n",
    "    denom = self.probNotFound()\n",
    "    remainder = abs(curr_belief - (num/denom))\n",
    "    self.knowledge[x][y].setBelief(num/denom)\n",
    "    for i in range(self.ls.dim):\n",
    "        for j in range(self.ls.dim):\n",
    "            if i == x and j == y:\n",
    "                continue\n",
    "            else:\n",
    "                temp = self.knowledge[i][j].getBelief()\n",
    "                self.knowledge[i][j].setBelief(temp + (temp*remainder)/(1-remainder))\n",
    "    return self.knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns a new belief for a specified cell and updates the beliefs about the rest of the map, as described earlier. \n",
    "\n",
    "**Note:** We do not update the knowledge base when the target is found (case 1 above), as this is trivial: if the target is found, the program will terminate anyway.\n",
    "\n",
    "The above method relies on the function `probNotFound` which computes $P(H|E)$ exactly as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probNotFound(self):\n",
    "    n = self.ls.dim\n",
    "    res = 0\n",
    "    coords = self.getVisited()\n",
    "    res = (n**2 - len(coords))/(n**2)\n",
    "    for coord in coords:\n",
    "        res += (self.ls.landscape[coord].getTerrain())*(self.knowledge[coord].getBelief())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more implementation details, see `Agent.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent search strategies\n",
    "\n",
    "Armed with this `updateBelief` function, we need to define how the agent will choose cells to query in order to search maps. We consider two rules for which cell the agent should query next:\n",
    "\n",
    "1. Query the cell with the highest belief, i.e. the probability that **the target is in that cell.**\n",
    "\n",
    "2. Query the cell with the highest probability that **the target will be found in such a query.**\n",
    "\n",
    "We implement both probability rules, and then use either of them to implement the agent's search algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 1: Prioritize the probability that the target is in a given cell\n",
    "\n",
    "This is based on the same $P(H|E)$ computed above. Upon each failed query, we update the entire knowledge base of beliefs as described above, and then we visit the cell with the highest belief. Its implementation is described above via `updateBelief`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 2: Prioritize the probability that the target will be found, if a given cell is searched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic intuition\n",
    "\n",
    "Observe that this probability is different from $\\text{Belief}_i$. It must consider the impact of the terrain's interference with queries, i.e. potential false negatives. We want the following probability:\n",
    "\n",
    "$$P(F) := P(\\text{Target found in Cell}_i | \\text{Observations}_t)$$\n",
    "\n",
    "which is equal to:\n",
    "\n",
    "$$(1 - P(\\text{Target not found in Cell}_i | \\text{Target is in Cell}_i)) * P(\\text{Target is in Cell}_i)$$\n",
    "\n",
    "Note that $P(\\text{Target is in Cell}_i)$ is exactly $P(H)$ from before, and $P(\\text{Target not found in Cell}_i | \\text{Target is in Cell}_i)$ is determined by the terrain type of $\\text{Cell}_i$, so we can easily compute $P(F)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The probability that the target will be found at a given cell if it is searched is computed by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probFound(self,x,y):\n",
    "        return (1-self.ls.landscape[x][y].getTerrain())*self.knowledge[x][y].getBelief()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent search implementation for both rules\n",
    "\n",
    "We drive the above probability-calculating and belief update functions with the below methods in the `agent` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLikCell(self, start_i, start_j):\n",
    "    if self.rule == 1:\n",
    "        #get max i for P(Target in Cell i)\n",
    "        belief = np.array([[self.knowledge[i][j].getBelief() for j in range(self.ls.dim)] for i in range(self.ls.dim)])\n",
    "        return np.unravel_index(belief.argmax(),belief.shape)\n",
    "    elif self.rule == 2:\n",
    "        #get max i for P(Target FOUND in Cell i)\n",
    "        belief = [[self.knowledge[i][j].getBelief()*(1-self.ls.landscape[i][j].getTerrain()) for j in range(self.ls.dim)] for i in range(self.ls.dim)]\n",
    "        belief = np.array(belief)\n",
    "        return np.unravel_index(belief.argmax(),belief.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will find the cell with maximum probability for either rule:\n",
    "\n",
    "1. Return the cell coordinates with the highest Rule 1 probability, i.e. belief. For arbitrary $\\text{Cell}_j$, this is given by:\n",
    "\n",
    "$$\\text{argmax}_j \\quad P_\\text{Rule1}(j)$$\n",
    "\n",
    "2. Return the cell coordinates with the highest Rule 2 probability, i.e. the equation in `probFound` above. For arbitrary $\\text{Cell}_j$, this is given by:\n",
    "\n",
    "$$\\text{argmax}_j \\quad P_\\text{Rule2}(j)$$\n",
    "\n",
    "In either case, ties between multiple maximum-probability cells are broken arbitrarily.\n",
    "\n",
    "Finally, the driver code which will repeat belief updates and getting the maximum-probability cell is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTarget(self):\n",
    "    i = self.i; j = self.j\n",
    "    self.num_actions += 1\n",
    "    while not self.searchCell(self.ls.landscape[i][j]):\n",
    "        self.knowledge = self.updateBelief(i,j)\n",
    "        next_i,next_j = self.getMaxLikCell(i,j)\n",
    "        self.num_actions += 1\n",
    "        i,j = next_i,next_j\n",
    "    return (i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will use the `i` and `j` fields of the `agent` class to start the search, corresponding to either randomized or user-inputted coordinates for the first cell to query. Then, for subsequent iterations, it will query the cell with the highest-probability (based on either rule), update all beliefs, and increment the number of actions incurred along the way. \n",
    "\n",
    "It terminates once the target is found. At termination, the agent will have recorded the number of actions taken to find the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison between both rules\n",
    "\n",
    "For all comparisons listed below between the two probability rules prioritized in `getMaxLikCell`, we run trials on map(s) of $dim = 50$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated trials over the same landscape\n",
    "\n",
    "For a fixed landscape, we run $n=200$ trials of Rule 1 agents and Rule 2 agents solving the same map, and we record the number of actions taken for each agent and trial. \n",
    "\n",
    "Note that for each trial, we start both agents' queries at the *same* cell, selected at random. Per trial, we also reset the location of the target to a new and different location, chosen uniformly at random via the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetTarget(self):\n",
    "    x = self.target_x; y = self.target_y\n",
    "    self.landscape[x][y].target = ABSENT\n",
    "    \n",
    "    new_targ_coords = list(set([(x,y) for x in range(self.dim) for y in range(self.dim)]).difference({(x,y)}))\n",
    "    \n",
    "    new_target = random.choice(new_targ_coords)\n",
    "    self.target_x = new_target[0]\n",
    "    self.target_y = new_target[1]\n",
    "    self.landscape[self.target_x][self.target_y].target = PRESENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the following randomly-generated landscape, where the target is **initially** at (41, 36).\n",
    "\n",
    "![Fixed Landscape Rule One v. Two Trials](./imgs/landscape_ruleOneTwoComparison.png)\n",
    "\n",
    "The comparison data generated is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RuleOne</th>\n",
       "      <td>4663</td>\n",
       "      <td>630</td>\n",
       "      <td>17309</td>\n",
       "      <td>4837</td>\n",
       "      <td>14650</td>\n",
       "      <td>10832</td>\n",
       "      <td>1801</td>\n",
       "      <td>26230</td>\n",
       "      <td>4480</td>\n",
       "      <td>2937</td>\n",
       "      <td>...</td>\n",
       "      <td>6816</td>\n",
       "      <td>2131</td>\n",
       "      <td>1591</td>\n",
       "      <td>10574</td>\n",
       "      <td>19533</td>\n",
       "      <td>1684</td>\n",
       "      <td>18856</td>\n",
       "      <td>896</td>\n",
       "      <td>21249</td>\n",
       "      <td>12430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleTwo</th>\n",
       "      <td>1877</td>\n",
       "      <td>129</td>\n",
       "      <td>1397</td>\n",
       "      <td>11290</td>\n",
       "      <td>1525</td>\n",
       "      <td>1009</td>\n",
       "      <td>369</td>\n",
       "      <td>7965</td>\n",
       "      <td>18998</td>\n",
       "      <td>29344</td>\n",
       "      <td>...</td>\n",
       "      <td>3951</td>\n",
       "      <td>436</td>\n",
       "      <td>318</td>\n",
       "      <td>750</td>\n",
       "      <td>607</td>\n",
       "      <td>6279</td>\n",
       "      <td>445</td>\n",
       "      <td>17597</td>\n",
       "      <td>4003</td>\n",
       "      <td>2309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1      2      3      4      5     6      7      8      9    \\\n",
       "RuleOne  4663  630  17309   4837  14650  10832  1801  26230   4480   2937   \n",
       "RuleTwo  1877  129   1397  11290   1525   1009   369   7965  18998  29344   \n",
       "\n",
       "         ...   190   191   192    193    194   195    196    197    198    199  \n",
       "RuleOne  ...  6816  2131  1591  10574  19533  1684  18856    896  21249  12430  \n",
       "RuleTwo  ...  3951   436   318    750    607  6279    445  17597   4003   2309  \n",
       "\n",
       "[2 rows x 200 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fixed_ruleOneTwoComp_df = pd.read_csv('./data/fixed_map_comparison_ruleOneTwo.csv')[['RuleOne', 'RuleTwo']]\n",
    "fixed_ruleOneTwoComp_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above trial-by-trial data on the number of searches required for the agents (using rules 1 or 2) to find the target can be visualized via the following scatter plot:\n",
    "\n",
    "![Rules 1-2 Comparison by Trials, Scatter](./imgs/fixed_map_comparison_ruleOneTwoScatter.png)\n",
    "\n",
    "It appears that the agent using Rule 1 requires a higher number of searches to find the target. But this pattern is not immediately clear. To mediate this, we consider the quantity:\n",
    "\n",
    "$$\\text{Diff} = \\text{Number of searches}_{\\text{Rule1}} - \\text{Number of searches}_{\\text{Rule2}}$$\n",
    "\n",
    "![Rules 1-2 Difference by Trials, Plot](./imgs/fixed_map_comparison_ruleOneTwoDiff.png)\n",
    "\n",
    "![Rules 1-2 Difference by Trials, Box](./imgs/fixed_map_comparison_ruleOneTwoDiffBox.png)\n",
    "\n",
    "The above images show that the difference between the number of searches required following rules 1 and 2 has high spread. The following 1-variable statistics describe the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>diff</th>\n",
       "      <td>200.0</td>\n",
       "      <td>1647.5</td>\n",
       "      <td>9229.267409</td>\n",
       "      <td>-32161.0</td>\n",
       "      <td>-2413.5</td>\n",
       "      <td>870.5</td>\n",
       "      <td>4922.0</td>\n",
       "      <td>36092.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count    mean          std      min     25%    50%     75%      max\n",
       "diff  200.0  1647.5  9229.267409 -32161.0 -2413.5  870.5  4922.0  36092.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixedmap_1varstats = pd.read_csv('./data/fixedmap_1varstats.csv', index_col=0)\n",
    "fixedmap_1varstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, for a fixed map, it appears that **the agent using Rule 2** outperforms the agent using Rule 1 in terms of, on average, **1648 fewer searches required to find the target.** However, the large variance visualized above strongly motivates repeated trials over multiple randomly-generated maps in order to see if this pattern holds in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated trials over multiple landscapes\n",
    "\n",
    "We conduct similar trials to compare both agents for $N = 50$ distinct and randomly generated maps. For each map and agent, we record the average number of actions taken to find the target over $n = 30$ trials. As above, we start both agents at the same initial queried cell (at random) and reset the target to a random new location in between each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RuleOne</th>\n",
       "      <td>6239.666667</td>\n",
       "      <td>8238.2</td>\n",
       "      <td>7995.200000</td>\n",
       "      <td>4642.600000</td>\n",
       "      <td>5201.333333</td>\n",
       "      <td>6096.200000</td>\n",
       "      <td>5929.8</td>\n",
       "      <td>3793.733333</td>\n",
       "      <td>6416.133333</td>\n",
       "      <td>4689.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>4645.466667</td>\n",
       "      <td>5928.066667</td>\n",
       "      <td>6189.0</td>\n",
       "      <td>3903.866667</td>\n",
       "      <td>6836.933333</td>\n",
       "      <td>4990.000000</td>\n",
       "      <td>4923.266667</td>\n",
       "      <td>7069.466667</td>\n",
       "      <td>8294.866667</td>\n",
       "      <td>7375.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleTwo</th>\n",
       "      <td>3687.333333</td>\n",
       "      <td>7271.0</td>\n",
       "      <td>6089.666667</td>\n",
       "      <td>4928.133333</td>\n",
       "      <td>8382.600000</td>\n",
       "      <td>6516.933333</td>\n",
       "      <td>5877.8</td>\n",
       "      <td>4493.466667</td>\n",
       "      <td>7009.933333</td>\n",
       "      <td>3859.933333</td>\n",
       "      <td>...</td>\n",
       "      <td>5404.133333</td>\n",
       "      <td>6142.333333</td>\n",
       "      <td>4896.2</td>\n",
       "      <td>6497.600000</td>\n",
       "      <td>7608.733333</td>\n",
       "      <td>5126.333333</td>\n",
       "      <td>5827.266667</td>\n",
       "      <td>7152.266667</td>\n",
       "      <td>9738.533333</td>\n",
       "      <td>7034.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0       1            2            3            4   \\\n",
       "RuleOne  6239.666667  8238.2  7995.200000  4642.600000  5201.333333   \n",
       "RuleTwo  3687.333333  7271.0  6089.666667  4928.133333  8382.600000   \n",
       "\n",
       "                  5       6            7            8            9   ...  \\\n",
       "RuleOne  6096.200000  5929.8  3793.733333  6416.133333  4689.800000  ...   \n",
       "RuleTwo  6516.933333  5877.8  4493.466667  7009.933333  3859.933333  ...   \n",
       "\n",
       "                  40           41      42           43           44  \\\n",
       "RuleOne  4645.466667  5928.066667  6189.0  3903.866667  6836.933333   \n",
       "RuleTwo  5404.133333  6142.333333  4896.2  6497.600000  7608.733333   \n",
       "\n",
       "                  45           46           47           48           49  \n",
       "RuleOne  4990.000000  4923.266667  7069.466667  8294.866667  7375.866667  \n",
       "RuleTwo  5126.333333  5827.266667  7152.266667  9738.533333  7034.200000  \n",
       "\n",
       "[2 rows x 50 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_ruleOneTwoComp_df = pd.read_csv('./data/multimap_comparison_ruleOneTwo.csv')[['RuleOne', 'RuleTwo']]\n",
    "multi_ruleOneTwoComp_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Rules 1-2 Comparison by Trials, Multi, Scatter](./imgs/multi_map_comparison_ruleOneTwoScatter.png)\n",
    "\n",
    "![Rules 1-2 Difference by Trials, Multi, Plot](./imgs/multi_map_comparison_ruleOneTwoDiff.png)\n",
    "\n",
    "![Rules 1-2 Difference by Trials, Multi, Box](./imgs/multi_map_comparison_ruleOneTwoDiffBox.png)\n",
    "\n",
    "As before, the difference distribution shown above has quite a large spread. The distribution can be described by the following one-variable descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>diff</th>\n",
       "      <td>50.0</td>\n",
       "      <td>259.573333</td>\n",
       "      <td>1722.672219</td>\n",
       "      <td>-3181.266667</td>\n",
       "      <td>-787.8</td>\n",
       "      <td>26.233333</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>4715.533333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count        mean          std          min    25%        50%     75%  \\\n",
       "diff   50.0  259.573333  1722.672219 -3181.266667 -787.8  26.233333  1290.0   \n",
       "\n",
       "              max  \n",
       "diff  4715.533333  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimap_1varstats = pd.read_csv('./data/multimap_1varstats.csv', index_col=0)\n",
    "multimap_1varstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that over multiple maps, **rule two** still outperforms rule one by ways of, on average, **260 fewer searches required to find the target.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "This makes intuitive sense, because rule two prioritizes a more realistic goal: *finding* the target versus trying to ordain where the target is. In a sense, the two rules are related, but rule two takes more directly into account the possibility of false negatives in a query. Instead of prioritizing where the target is, but possibly still getting a false negative in response, the rule two-prioritizing agent focuses on the most revealing queries. This is more \"intelligent\" in the sense that the agent's queries are more effective in driving belief updates, and this yields a lower number of searches required to find the target.\n",
    "\n",
    "Of course, these data does not seem like a decisive statistical victory for rule two. Accounting for multiple maps has reduced the standard deviation in the difference distribution from 8418 to 1722, but we see that a few outliers still have a pronounced effect on the distributions described above. After running trials on multiple maps, we do see that at best rule two outperforms rule one by 4715 fewer searches, and at worst rule one outperforms rule two by 3181 fewer searches; one might say that in both extreme worst case scenarios, rule two still prevails.\n",
    "\n",
    "But even though we hold maps fixed over repeated trials, variations in the data are naturally likely a result of:\n",
    "\n",
    "- Randomly selecting a first cell to query\n",
    "- Probabalistic revealing of false negatives\n",
    "\n",
    "As expected from the problem statement, every trial for the same map will vary in terms of how the agent will solve, which sequence of cells to query, etc. because the environment is probabilistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted agent movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we consider an agent for which movement between one queried cell to the next **matters.** \n",
    "\n",
    "That is, we consider movement of one cell in any direction to be an action that the agent takes, just like querying is considered as one action. For example, if an agent has just queried (0,0) and then wishes to query (5,5), it will need to navigate 5 cells down and 5 cells right, meaning 10 total cells traveled across. Then, the next query at (5,5)  counts as an action, making the total number of additional actions, assuming querying (0,0) has already been counted, is $10 + 1 = 11$.\n",
    "\n",
    "Note that we are using the **Manhattan distance** to consider travel costs, and not the Euclidean distance, because we assume the agent can only travel rectilinearlly, i.e. up, down, right, or left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 3: Run with Rule 2, but also incur action costs for agent travel\n",
    "\n",
    "As described above, we define a **Rule 3** agent to be exactly the same as a Rule 2 agent, not a Rule 1 agent, as we found Rule 2 to perform better. However, we additionally modify the incrementing of the number of actions required as follows:\n",
    "\n",
    "In addition to counting individual cell queries as one action each, also calculate the **Manhattan distance** between subsequently queried cells:\n",
    "\n",
    "$$\\text{dist}((x_1, y_1), (x_2, y_2)) = | x_1 - x_2 | + | y_1 - y_2 |$$\n",
    "\n",
    "Then, multiply this quantity by one action per distance unit and add this to the total number of actions incremented at each time step.\n",
    "\n",
    "The implementation simply adds on a case (which also works for rule 4 below) for additional incrementation for the `findTarget` method in the `agent` described previously: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTarget(self):\n",
    "    # ... previous code, which also increments by 1 for queries\n",
    "    if self.rule == 2 or self.rule == 3:\n",
    "        self.num_actions += math.abs(i - next_i) + math.abs(j - next_j)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and adds on a case in `getMaxLikCell` which is identical to the case for rule 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLikCell(self, start_i, start_j):\n",
    "    # ... previous code for rules 1, 2\n",
    "    elif self.rule == 3:\n",
    "        belief = [[self.knowledge[i][j].getBelief()*(1-self.ls.landscape[i][j].getTerrain()) for j in range(self.ls.dim)] for i in range(self.ls.dim)]\n",
    "        belief = np.array(belief)\n",
    "        return np.unravel_index(belief.argmax(),belief.shape)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naively, running a rule 3 agent on maps will solve them just as a rule 2 agent would, except it would incur much higher values for the number of actions required to find the target. That is, rule 3 is an unintelligent approach to solving this distance-based problem.\n",
    "\n",
    "To mediate the effects of this performance hit, we need to re-adapt our agent's priorities in order to account for necessary travel distance as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 4: Prioritize cells both on Rule 2 and travel costs\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Instead of using the Rule 1 or Rule 2 probabilities to prioritize the next cell queried, we instead devise a new heuristic which depends only on the cell's Rule 2 probability (of finding the target) as well as the travel cost of reaching that cell from the current one.\n",
    "\n",
    "For simplicity of notation, let the previous Rule 2 probability for any cell $i$ to be denoted as $p_i$. \n",
    "\n",
    "If the most-recently queried cell is $\\text{Cell}_i$, then let $m_{ij}$ be the total number of actions required to query any other $\\text{Cell}_j$ where we **do not require** $j \\neq i$. This is equal to Manhattan distance between $\\text{Cell}_i$ and  $\\text{Cell}_j$ plus 1 for the cost of querying $\\text{Cell}_j$.\n",
    "\n",
    "Next, let $k_t$ be the number of actions at time $t$ to end game, i.e. the number of queries and/or movements left in the game prior to finding the target. Then, the probability of not finding the target prior to end game, if we query $\\text{Cell}_j$ next after $\\text{Cell}_i$, is given by:\n",
    "\n",
    "$$P(\\text{\\{find target only at end game\\}}) = (1-p_i)^{\\frac{k_t}{m_{ij}}}$$\n",
    "\n",
    "\n",
    "We intuitively want to minimize this probability at each time step such that we can find the target in fewer than $k$ steps, i.e. before end game. To accomplish this, at each time step and regardless of the value of $k_t$, we choose the next cell to query after $\\text{Cell}_i$ as:\n",
    "\n",
    "$$\\text{argmin}_j \\quad \\{ h(i,j) = (1-p_i)^{\\frac{1}{m_{ij}}} \\}$$\n",
    "\n",
    "That is, we choose the $\\text{Cell}_j$ such that we minimize the heuristic $h(i,j)$, which relates the rule 2 probability and the distance between $\\text{Cell}_i$, $\\text{Cell}_j$ in an **exponential relationship.**\n",
    "\n",
    "That way, very high distances from the current cell and very low rule 2 probabilities penalize the heuristic value by increasing it, and vice versa, which is exactly what we wish our rational rule 3 agent to do.\n",
    "\n",
    "Implementation-wise, this is nothing more than an alternative heuristic to rules 1 and 2 to prioritize cells to query at each time step, except we wish to **minimize** this heuristic, unlike the probabilities in rules 1 and 2, which we sought to maximize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The rule 4 agent simply adds on a case for `getMaxLikCell` for rule 4 to calculate the argmin of the exact heuristic described above, and then it proceeds as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLikCell(self, start_i, start_j):\n",
    "    # ... previous code for rules 1-3\n",
    "    else:\n",
    "        belief = [[math.pow(1 - self.knowledge[i][j].getBelief()*(1-self.ls.landscape[i][j].getTerrain()), 1/(1 + math.abs(start_i - i) + math.abs(start_j - j)) ) \\\n",
    "                for j in range(self.ls.dim)] for i in range(self.ls.dim)]\n",
    "        belief = np.array(belief)\n",
    "        return np.unravel_index(belief.argmin(),belief.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Again, we are finding the argmin of the heuristic and not the argmax of the likelihood (as for rules 1 and 2), as the method name suggests. This is for compatibility with the driver `findTarget` method.\n",
    "\n",
    "**Note 2:** Recall that for rule 3, we added a case in `findTarget` for both rules 3 and 4 to additionally increment the number of agent actions by the Manhattan distance travelled. This holds here as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison between rules 3 and 4\n",
    "\n",
    "When comparing agents using rules 1 and 2, we found that the multi-map comparison controlled for variance better than the fixed-map comparison. We also found it useful to consider the difference in number of actions taken to find the target, i.e.\n",
    "\n",
    "$$\\text{Diff} = \\text{Number of searches}_{\\text{Rule3}} - \\text{Number of searches}_{\\text{Rule4}}$$\n",
    "\n",
    "We repeat this particular trial and comparison methodology here for agents using rules 3 and 4, generating $N = 20$ distinct $dim=50$ maps and obtaining the average (over $n=10$ trials per map) number of actions taken to find the target for both rules.\n",
    "\n",
    "As before, we start both agents at the same initial queried cell (at random) and reset the target to a random new location in between each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RuleThree</th>\n",
       "      <td>29329.1</td>\n",
       "      <td>36727.9</td>\n",
       "      <td>32621.4</td>\n",
       "      <td>33314.4</td>\n",
       "      <td>46540.8</td>\n",
       "      <td>49841.5</td>\n",
       "      <td>33185.1</td>\n",
       "      <td>73028.2</td>\n",
       "      <td>39707.6</td>\n",
       "      <td>41723.4</td>\n",
       "      <td>30311.6</td>\n",
       "      <td>26156.6</td>\n",
       "      <td>33825.5</td>\n",
       "      <td>62614.2</td>\n",
       "      <td>49114.8</td>\n",
       "      <td>42106.8</td>\n",
       "      <td>29039.9</td>\n",
       "      <td>47265.5</td>\n",
       "      <td>48582.7</td>\n",
       "      <td>40435.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleFour</th>\n",
       "      <td>12272.3</td>\n",
       "      <td>10802.8</td>\n",
       "      <td>14107.1</td>\n",
       "      <td>8385.6</td>\n",
       "      <td>24289.0</td>\n",
       "      <td>9705.1</td>\n",
       "      <td>6275.2</td>\n",
       "      <td>12444.3</td>\n",
       "      <td>6366.4</td>\n",
       "      <td>6893.0</td>\n",
       "      <td>11876.3</td>\n",
       "      <td>11679.4</td>\n",
       "      <td>4315.3</td>\n",
       "      <td>5042.9</td>\n",
       "      <td>9460.1</td>\n",
       "      <td>14121.0</td>\n",
       "      <td>9060.1</td>\n",
       "      <td>5975.7</td>\n",
       "      <td>7064.3</td>\n",
       "      <td>9834.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0        1        2        3        4        5        6   \\\n",
       "RuleThree  29329.1  36727.9  32621.4  33314.4  46540.8  49841.5  33185.1   \n",
       "RuleFour   12272.3  10802.8  14107.1   8385.6  24289.0   9705.1   6275.2   \n",
       "\n",
       "                7        8        9        10       11       12       13  \\\n",
       "RuleThree  73028.2  39707.6  41723.4  30311.6  26156.6  33825.5  62614.2   \n",
       "RuleFour   12444.3   6366.4   6893.0  11876.3  11679.4   4315.3   5042.9   \n",
       "\n",
       "                14       15       16       17       18       19  \n",
       "RuleThree  49114.8  42106.8  29039.9  47265.5  48582.7  40435.2  \n",
       "RuleFour    9460.1  14121.0   9060.1   5975.7   7064.3   9834.5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_ruleThreeFourComp_df = pd.read_csv('./data/multimap_comparison_ruleThreeFour.csv')[['RuleThree', 'RuleFour']]\n",
    "multi_ruleThreeFourComp_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Rules 3-4 Comparison by Trials, Multi, Scatter](./imgs/multi_map_comparison_ruleThreeFourScatter.png)\n",
    "\n",
    "![Rules 1-2 Difference by Trials, Multi, Plot](./imgs/multi_map_comparison_ruleThreeFourDiff.png)\n",
    "\n",
    "![Rules 1-2 Difference by Trials, Multi, Box](./imgs/multi_map_comparison_ruleThreeFourDiffBox.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>diff</th>\n",
       "      <td>20.0</td>\n",
       "      <td>31275.09</td>\n",
       "      <td>12658.964923</td>\n",
       "      <td>14477.2</td>\n",
       "      <td>21683.8</td>\n",
       "      <td>28748.0</td>\n",
       "      <td>39775.125</td>\n",
       "      <td>60583.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean           std      min      25%      50%        75%  \\\n",
       "diff   20.0  31275.09  12658.964923  14477.2  21683.8  28748.0  39775.125   \n",
       "\n",
       "          max  \n",
       "diff  60583.9  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimap_1varstats_ruleThreeFour = pd.read_csv('./data/multimap_1varstats_ruleThreeFour.csv', index_col=0)\n",
    "multimap_1varstats_ruleThreeFour.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encouragingly, we see that **Rule 4 strictly outperforms Rule 3**, i.e. naively running with Rule 2, by a performance improvement of, on average, **31,275 fewer actions taken.** This includes costs of both queries and traveling.\n",
    "\n",
    "The distribution of the performance improvement does still hvae a large spread, with improvements ranging from 14,477 fewer to as many as 60,584 fewer actions taken to find the target. However, the standard deviation is significantly lower than the mean, and in any case, all trials showed improvement over the baseline rule 3 agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improvements via utility\n",
    "\n",
    "Our strategy with the Rule 4 agent leverages the theory of utility in the sense that it balances negative utility from traveling (which raises the $h(i,j)$ value described earlier) with positive utility from potentially finding targets at cells (which lowers the $h(i,j)$ value described earlier). \n",
    "\n",
    "With the exponential weighting on the traveling cost, one could say our heuristic is more sensitive to changes in the traveling cost than it is to changes in the rule 2 probability. However, one could also say this **prioritizes immediate rewards over future rewards.**\n",
    "\n",
    "That is, what about scenarios in which the agent might sacrifice some immediate utility by traveling further for higher rule 2 probability-cells instead of querying closer cells with lower rule 2 probabilities? What about if there were a cluster of many cells with rule 2 probability 0.6 all within 5 travel units away, and one cell with rule 2 probability 0.9 that was 15 travel units away? The former case would have heuristic $(1-0.6)^{\\frac{1}{6}} = 0.8584$, and the latter case would have heuristic $(1-0.1)^{\\frac{1}{16}} = 0.8660$. Assuming we only care about these two possibilities, our agent would choose to travel closer for the immediate but lower potential reward, instead of risking a long travel path for a higher potential reward.\n",
    "\n",
    "Our rule 4 agent does not account for utility of clusters or total utilities of cells traveled along a path to reach the cell queried. Further improvements might build atop the 'naive' utility function in rule 4 in order to account for these factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A drunk man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This old joke shines a light at a major problem with a lot of different algorithmic strategies to solve this search and destroy problem. The biggest problem in solving this assignment was that you could never be perfectly sure of a “true false.” Everything was based off probabilities. To that point, there were certain terrains that had lower probabilities of having a false negative. In the order of lowest probabilities of false negatives to highest: flat, hilly, forested, maze of caves.  When we would present probabilities to our probabilistic inference model using Bayes’ Theorem, probabilities of a false negative for flat was a lot smaller then that of a maze of caves. The intuition with algorithms like these is trying to gauge targets at flat or hilly surfaces before forested or a maze of caves terrains since its probability of finding a false negative was a lot higher. \n",
    "\n",
    "After collecting our own data on the Rule 1 vs Rule 2 comparison, we were able to justify the decision of the drunk man (jokingly). After running the implementations of each against one another, we were able to deduce on average, on multiple landscapes and on a fixed landscape, Rule 2 outperforms Rule 1 by at least a few hundred searches. In our own implementation, it seems that searching cells of terrains that have low false negative rates are better to search before searching terrains that may have high probabilities of containing the target, regardless of its terrain. \n",
    "\n",
    "There were cases where Rule 1 outperformed Rule 2 on certain terrains, but even at its worst case Rule 1 seemed to still not perform terribly worse than Rule 2. So even at its best and worst, Rule 2 seemed to be the winning algorithm in terms of success rates and efficiency. \n",
    "\n",
    "What also becomes known is that there will always be outliers with both rules that require either an extremely high amount of searches or an incredibly low amount of searches. Even though, the distribution on random maps over a fixed map seemed to lower the standard deviation significantly (i.e. 8418 searches vs 1722 searches), this is still a rather high standard deviation in itself. Variations for a search and destroy algorithm of this sort are always going to be rather large. \n",
    "\n",
    "The joke demonstrates how a drunk man (seemingly for this problem a low efficiency AI) seems to believe that it’s better to search somewhere where it’s easier to find a target (under the streetlight) rather than search areas where there is a higher rate of success (parking lot). But according to our findings from Rule 1 vs Rule 2 efficiency, it almost seems as if the drunk man has a point. Obviously, in the joke, the drunk man is wrong. But in truth for false negative target searching, it may in fact be better to search places where there is a higher probability of finding the target before searching places where this is a higher probability of containing a target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A moving target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider a target which is not static in that:\n",
    "\n",
    "1. Upon every failed query, the target moves to one of its neighbors at random.\n",
    "2. Upon moving to one of its neighbors, some sensor returns an observation of terrain type (\"FLAT\" , \"HILLY\", etc.). However, the sensor is broken, so it returns some type that the new target is **not,** at random. \n",
    "\n",
    "For example, if we query a cell and do not find the target, and we get the new observation \"HILLY\", then we know that the target is now in a cell which is either \"FLAT\", \"FOREST\", or \"MAZE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Due to the object-oriented nature of our `Landscape` class, the target can easily be moved.\n",
    "\n",
    "The below code moves the target to one of its neighbors at random, and returns a terrain type the new target is not, at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveTarget(self):\n",
    "    x = self.target_x; y = self.target_y\n",
    "    self.landscape[x][y].target = ABSENT\n",
    "\n",
    "    nbrs = []\n",
    "    for dx, dy in dirs:\n",
    "        if 0 <= x + dx < self.dim and 0 <= y + dy < self.dim:\n",
    "            nbrs.append((x + dx, y + dy))\n",
    "\n",
    "    new_nbr = random.choice(nbrs)\n",
    "    self.target_x = new_nbr[0]; self.target_y = new_nbr[1]\n",
    "    self.landscape[self.target_x][self.target_y].target = PRESENT\n",
    "\n",
    "    terrains = {FLAT, HILLY, FOREST, MAZE}\n",
    "    new_terrain = {(self.landscape[self.target_x][self.target_y]).terrain}\n",
    "    return random.choice(list(terrains.difference(new_terrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy\n",
    "\n",
    "With a moving target, our first problem that arrives is that we have no true knowledge base of where the target will move after each iteration. In our stationary target search, there is an eventual end where we will find the target since it must exist in one of the squares of the grid. So in the worst case scenario it will take dim x dim searches to find the target. In a moving target scenario, this is not the case. There is a possibility that the target will never be found. \n",
    "\n",
    "To deal with this problem, we are provided with the information of the tracker. The tracker tells us with 100% assurance what terrain the target will not be in. So there is a 1/3 likeliness it exists in either of the other three terrains. With this information we can knock out 1/4 of the cells (this isn’t always 1/4, it depends on the random allocation according to the given probabilities). After this, we are also provided with the information as to where the target moved next since it only moves to its neighbors. \n",
    "\n",
    "At the base step, we are provided with a matrix that has 1/(total cells) probability in each cell. After making our first random guess, we can update the table according to our original strategy of obtaining probabilities. Now, we have the probabilities of each of the given cells according to the probabilities we attained through our normal means using Bayes Theorem. But now, we will use the information we receive from the tracker to remove cells of the terrain that the tracker reported since we know for certain they do not contain the target. Now, we have a reduced cell matrix. We choose the cell with the highest probability of containing the cell. \n",
    "\n",
    "Now, after our first guess reports a failed search - the target has moved. We are told that the target has moved to a neighbor of its previous cell. Since we are not sure where the target was, we cannot assume its neighbors. But what we can do is look into the neighbors of the previous cell and lower some of their probabilities depending on if they have neighbors from the opposite side or not. For example, if the cell is neighbors with a corner cell then you can lower the probability of both the cell and its neighbor since there is no other way of reaching that cell other than through the suspected target cell. \n",
    "\n",
    "A lot of this information that we now have access to is useful in the sense we are better off with this information than not and we can leverage the information to find a certain target faster. But, I am not sure how much better the tracker information really is compared to not having it. There isn’t too much use into the tracker since we can never truly be sure where the target is so we must make many assumptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A moving target AND an agent with restricted movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
