{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Search and Destroy\n",
    "\n",
    "Authors:\n",
    "\n",
    "- Rohan Rele (rsr132)\n",
    "- Aakash Raman (abr103)\n",
    "- Alex Eng (ame136)\n",
    "- Adarsh Patel (aap237)\n",
    "\n",
    "This project was completed for Professor Wes Cowan's Fall 2019 offering of the CS 520: Intro to Artificial Intelligence course, taught at Rutgers University, New Brunswick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we consider a two-dimensional map of cells in which one cell is randomly designated as the target. The location of the target is not known to any solving agent. Therefore, the problem is to devise an agent which can effectively query the landscape of cells, contribute towards its knowledge base based on observations, and ultimately find the target in the **minimal number of queries.**\n",
    "\n",
    "The knowledge base itself will contain probabilistic knowledge, i.e. \n",
    "\n",
    "$$\\text{Belief}[\\text{Cell}_i] = P(\\text{Target in Cell}_i|  \\text{Observations through time } t)$$ \n",
    "\n",
    "For every cell, this is the probability that a given cell contains the target given the existing knowledge base. Initially, as the agent has no prior knowledge about the map, the belief for each cell is $\\frac{1}{dim^2}$.\n",
    "\n",
    "Each cell also contains a terrain type which corresponds to the probability that a query will return a false negative, i.e.\n",
    "\n",
    "$$P(\\text{Target not found in Cell}_i | \\text{Target is in Cell}_i)$$\n",
    "\n",
    "which is $0.1$ for **flat** terrain cells, $0.3$ for **hilly** terrain cells, $0.7$ for **forested** terrain cells, and $0.9$ for cells whose terrain is a maze of **caves.**\n",
    "\n",
    "We assume that for any given map, each cell is assigned the flat terrain type with probability $0.2$, the hilly terrain type with probability $0.3$, the forested terrain type with probability $0.3$, and the caves terrain type with probability $0.2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landscape\n",
    "\n",
    "We implement the landscape as a class, which has the following fields:\n",
    "\n",
    "- `dim` (int): the dimension of the $dim$ by $dim$ map\n",
    "- `landscape` (2D list) of `landCell` objects, each of which tracks:\n",
    "    - `target` (int): `PRESENT = 1` if this cell is the target, or `ABSENT = 0` otherwise\n",
    "    - `terrain` (int): `FLAT = 0.1` if this cell has flat terrain, `HILLY = 0.3`, `FOREST = 0.7`, or `MAZE = 0.9`, etc.\n",
    "- `target_x` (int): the x-coordinate of the target cell\n",
    "- `target_y` (int): the y-coordinate of the target cell\n",
    "\n",
    "A landscape is initialized with non-target cells that are assigned terrain types based on the probabilities previously described. It then randomly selects one cell to be the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class landscape:\n",
    "\n",
    "    dim = 0\n",
    "    landscape = [[]]\n",
    "    target_x = 0\n",
    "    target_y = 0\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.landscape = [[landCell() for _ in range(self.dim)] for _ in range(self.dim)]\n",
    "\n",
    "        target_x = random.randint(0, dim - 1)\n",
    "        target_y = random.randint(0, dim - 1)\n",
    "        self.landscape[target_x][target_y].target = PRESENT\n",
    "        self.target_x = target_x\n",
    "        self.target_y = target_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more implementation details, see `Landscape.py`.\n",
    "\n",
    "The `landCell` object is also defined in a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class landCell:\n",
    "\n",
    "    def __init__(self):\n",
    "        x = random.randint(1, 100)\n",
    "        self.target = ABSENT\n",
    "        if x <= 20:\n",
    "            self.terrain = FLAT\n",
    "        elif 20 < x <= 50:\n",
    "            self.terrain = HILLY\n",
    "        elif 50 < x <= 80:\n",
    "            self.terrain = FOREST\n",
    "        else:\n",
    "            self.terrain = MAZE\n",
    "\n",
    "    def getTerrain(self):\n",
    "        return self.terrain\n",
    "\n",
    "    def isTarget(self):\n",
    "        return (self.target==PRESENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more implementation details, see `Cell.py`.\n",
    "\n",
    "For example, an initialized $dim = 50$ landscape may look like this:\n",
    "\n",
    "![Blank Landscape](./imgs/landscape_blankTest.png)\n",
    "\n",
    "where the target is located at (40, 48)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement the agent as a class, which has the following fields:\n",
    "\n",
    "- `knowledge` (2D list) of `agentCell` objects, each of which tracks:\n",
    "    - `belief` (float): the probability that a given cell contains the target, as described above; initially $1/{dim}^2$\n",
    "    - `status` (boolean): either `VISITED = True` or `UNVISITED = False` depending on whether or not the cell has been queried previously; initially `False`\n",
    "- `rule` (int): either 1 or 2, corresponding to the two probability rules described below\n",
    "- `num_actions` (int): the number of actions, whether queries or movements (in the later case of a movement-restricted agent), executed so far; initially 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    num_actions = 0\n",
    "\n",
    "    def __init__(self, landscape, rule):\n",
    "        self.ls = landscape\n",
    "        d = self.ls.dim\n",
    "\n",
    "        if rule == 1 or rule == 2:\n",
    "            self.rule = rule\n",
    "        else:\n",
    "            print(\"Invalid rule, set to 1 by default\")\n",
    "            self.rule = 1\n",
    "\n",
    "        self.knowledge = [[agentCell() for j in range(d)] for i in range(d)]\n",
    "        for i in range(d):\n",
    "            for j in range(d):\n",
    "                self.knowledge[i][j].setBelief(1/(d**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more implementation details, see `Agent.py`.\n",
    "\n",
    "The `agentCell` object is also defined in a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agentCell:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.belief = 0\n",
    "        self.status = UNVISITED\n",
    "\n",
    "    def getBelief(self):\n",
    "        return self.belief\n",
    "\n",
    "    def getStatus(self):\n",
    "        return self.status\n",
    "\n",
    "    def setBelief(self,belief):\n",
    "        self.belief = belief\n",
    "\n",
    "    def setStatus(self,status):\n",
    "        self.status = status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more implementation details, see `Cell.py`.\n",
    "\n",
    "For example, an initialized agent knowledge base with $dim = 50$ landscape may look like this:\n",
    "\n",
    "![Blank Beliefs](./imgs/belief_blankTest.png)\n",
    "\n",
    "where each cell has initial belief $\\frac{1}{50^2} = 0.0004$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `agent` class also has the following methods (non-exhaustive list):\n",
    "\n",
    "- `searchCell(cell)` (boolean): query a `landCell` object. If it is not the target, return `False`. If it is the target, then only return `True` with probability $p = 1 - P(\\text{false negative})$, where the false negative probability depends on that cell's terrain type as described previously. Otherwise, return `False`. \n",
    "\n",
    "**Note:** In either case, the number of actions taken by the agent is incremented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchCell(self,cell): #search a landCell\n",
    "    self.num_actions += 1\n",
    "    \n",
    "    if not cell.isTarget():\n",
    "        return False\n",
    "    else:\n",
    "        p = 1 - cell.getTerrain()\n",
    "        if random.uniform(0, 1) < p:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `getVisited()` (list): return a list of all (x,y) coordinates which the agent has already queried at a given point in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVisited(self):\n",
    "    n = self.ls.dim\n",
    "    coords = []\n",
    "    for x in range(n):\n",
    "        for y in range(n):\n",
    "            if self.knowledge[x][y].getStatus():\n",
    "                coords.append((x,y))\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the belief state\n",
    "\n",
    "We require a method to update the belief state given the results of a query. There are two cases: \n",
    "\n",
    "1. A query of a cell found the target, in which case the belief for this cell is set to 1, and the beliefs for all other cells are set to 0.\n",
    "\n",
    "2. A query of a cell did not find the target, in which case the belief for this cell must be adjusted considering the probability that the query returned a false negative.\n",
    "\n",
    "The latter case considers the probability \n",
    "\n",
    "$$P(\\text{Target in Cell}_i | \\text{Observations}_t \\land \\text{Failure in Cell}_j)$$\n",
    "\n",
    "and relies on the probabilistic knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $H := \\{\\text{Target in Cell}_i\\}$ and $E := \\{\\text{Target not found if we queried every cell}\\}$. \n",
    "\n",
    "Then we want $P(H|E)$, or the probability that the target is in a cell given we have not found the target in any of our queries. We would like to compute this upon a failed query of a cell and accept this quantity as that cell's new belief.\n",
    "\n",
    "By **Bayes' theorem,** this quantity is:\n",
    "\n",
    "$$P(H|E) = \\frac{P(E|H)P(H)}{P(E)}$$\n",
    "\n",
    "Observe that $P(E|H)$ is the probability that the target is not found given the target is in the cell, which is exactly the false negative probability described above per terrain type. And $P(H)$ is exactly the agent's belief for that cell in the previous time step.\n",
    "\n",
    "One can see that observing a failed query for a given cell will decrease our belief that this cell contains the target, but this decrease is scaled by the possibility of false negatives.\n",
    "\n",
    "$P(E)$ is calculated with the following function: \n",
    "\n",
    "$$P(E) = \\sum_{i \\text{ visited}} P(\\text{Target not found in Cell}_i | \\text{Target is in Cell}_i) + \\sum_{j \\text{ unvisited}} P(\\text{Target not in Cell}_j)$$\n",
    "\n",
    "That is, it is the probability that some queried cell was a false negative and that the unqueried cells do not contain the target. In this situation, querying all remaining cells would not lead to us finding the target.\n",
    "\n",
    "Finally, once a queried cell is updated, we must update the rest of the knowledge base. \n",
    "\n",
    "Let $R_i = |{\\text{new belief of Cell}_i} - {\\text{old belief of Cell}_i}|$, or the difference between the new and old beliefs of the queried cell.\n",
    "\n",
    "Then for all remaining cells $j$, use the following update formula: \n",
    "\n",
    "$$\\text{Belief}^{t+1}_j = \\text{Belief}^t_j + \\frac{\\text{Belief}^t_j * R_i}{1 - R_i}$$\n",
    "\n",
    "This, in a sense, scales the previous belief by how much our query impacted the belief of the queried cell. One can see how failed queries will increase the beliefs of all other cells per iteration, although this increase may be marginal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Based on the intuition above, the implementation of a belief update is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateBelief(self,x,y):\n",
    "    if self.searchCell(self.ls.landscape[x][y]):\n",
    "        for i in range(self.ls.dim):\n",
    "            for j in range(self.ls.dim):\n",
    "                self.knowledge[i][j].setBelief(0)\n",
    "        self.knowledge[x][y].setBelief(1)\n",
    "    else:\n",
    "        #P(H|E) = P(E|H)P(H)/P(E)\n",
    "        #H: Target in cell\n",
    "        #E: Target not found\n",
    "        curr_belief = self.knowledge[x][y].getBelief()\n",
    "        num = self.ls.landscape[x][y].getTerrain()*curr_belief\n",
    "        denom = self.probNotFound()\n",
    "        remainder = abs(curr_belief - (num/denom))\n",
    "        self.knowledge[x][y].setBelief(num/denom)\n",
    "        for i in range(self.ls.dim):\n",
    "            for j in range(self.ls.dim):\n",
    "                if i == x and j == y:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp = self.knowledge[i][j].getBelief()\n",
    "                    self.knowledge[i][j].setBelief(temp + (temp*remainder)/(1-remainder))\n",
    "\n",
    "    return self.knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns a new belief for a specified cell and updates the beliefs about the rest of the map, as described earlier. It relies on the function `probNotFound` which computes $P(H|E)$ exactly as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probNotFound(self):\n",
    "    n = self.ls.dim\n",
    "    res = 0\n",
    "    coords = self.getVisited()\n",
    "    res = (n**2 - len(coords))/(n**2)\n",
    "    for coord in coords:\n",
    "        res += (self.ls.landscape[coord].getTerrain())*(self.knowledge[coord].getBelief())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more implementation details, see `Agent.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent search strategies\n",
    "\n",
    "Armed with this `updateBelief` function, we need to define how the agent will choose cells to query in order to search maps. We consider two rules for which cell the agent should query next:\n",
    "\n",
    "1. Query the cell with the highest belief, i.e. the probability that **the target is in that cell.**\n",
    "\n",
    "2. Query the cell with the highest probability that **the target will be found in such a query.**\n",
    "\n",
    "We implement both probability rules, and then use either of them to implement the agent's search algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 1: Probability that the target is in a given cell\n",
    "\n",
    "This is based on the same $P(H|E)$ computed above. Upon each failed query, we update the entire knowledge base of beliefs as described above, and then we visit the cell with the highest belief. Its implementation is described above via `updateBelief`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 2: Probability that the target will be found, if a given cell is searched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic intuition\n",
    "\n",
    "Observe that this probability is different from $\\text{Belief}_i$. It must consider the impact of the terrain's interference with queries, i.e. potential false negatives. We want the following probability:\n",
    "\n",
    "$$P(F) := P(\\text{Target found in Cell}_i | \\text{Observations}_t)$$\n",
    "\n",
    "which is equal to:\n",
    "\n",
    "$$(1 - P(\\text{Target not found in Cell}_i | \\text{Target is in Cell}_i)) * P(\\text{Target is in Cell}_i)$$\n",
    "\n",
    "Note that $P(\\text{Target is in Cell}_i)$ is exactly $P(H)$ from before, and $P(\\text{Target not found in Cell}_i | \\text{Target is in Cell}_i)$ is determined by the terrain type of $\\text{Cell}_i$, so we can easily compute $P(F)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The probability that the target will be found at a given cell if it is searched is computed by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probFound(self,x,y):\n",
    "        return (1-self.ls.landscape[x][y].getTerrain())*self.knowledge[x][y].getBelief()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent search implementation for both rules\n",
    "\n",
    "We drive the above probability-calculating and belief update functions with the below methods in the `agent` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLikCell(self):\n",
    "    if self.rule == 1:\n",
    "        #get max i for P(Target in Cell i)\n",
    "        belief = np.array([[self.knowledge[i][j].getBelief() for j in range(self.ls.dim)] for i in range(self.ls.dim)])\n",
    "        return np.unravel_index(belief.argmax(),belief.shape)\n",
    "    else:\n",
    "        #get max i for P(Target FOUND in Cell i)\n",
    "        belief = [[self.knowledge[i][j].getBelief()*(1-self.ls.landscape[i][j].getTerrain()) for j in range(self.ls.dim)] for i in range(self.ls.dim)]\n",
    "        belief = np.array(belief)\n",
    "        return np.unravel_index(belief.argmax(),belief.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will find the cell with maximum probability for either rule:\n",
    "\n",
    "1. Return the cell coordinates with the highest Rule 1 probability, i.e. belief.\n",
    "2. Return the cell coordinates with the highest Rule 2 probability, i.e. the equation in `probFound` above.\n",
    "\n",
    "In either case, ties between multiple maximum-probability cells are broken arbitrarily.\n",
    "\n",
    "Finally, the driver code which will repeat belief updates and getting the maximum-probability cell is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTarget(self):\n",
    "    i = random.randint(0, self.ls.dim-1); j = random.randint(0, self.ls.dim-1)\n",
    "    while not self.searchCell(self.ls.landscape[i][j]):\n",
    "        self.knowledge = self.updateBelief(i,j)\n",
    "        i,j = self.getMaxLikCell()\n",
    "    return (i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will randomly select a cell to query for the first iteration. Then, for subsequent iterations, it will query the cell with the highest-probability (based on either rule), and update all beliefs along the way. It terminates once the target is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison between both rules\n",
    "\n",
    "For all comparisons listed below between the two probability rules prioritized in `getMaxLikCell`, we run trials on map(s) of $dim = 50$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated trials over the same landscape\n",
    "\n",
    "For a fixed landscape, we run $n=200$ trials of Rule 1 agents and Rule 2 agents solving the same map, and we record the number of actions taken for each agent and trial. Note that for each trial, we also reset the location of the target to a new and different location, chosen uniformly at random via the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetTarget(self):\n",
    "    x = self.target_x; y = self.target_y\n",
    "    self.landscape[x][y].target = ABSENT\n",
    "    \n",
    "    new_targ_coords = list(set([(x,y) for x in range(self.dim) for y in range(self.dim)]).difference({(x,y)}))\n",
    "    \n",
    "    new_target = random.choice(new_targ_coords)\n",
    "    self.target_x = new_target[0]\n",
    "    self.target_y = new_target[1]\n",
    "    self.landscape[self.target_x][self.target_y].target = PRESENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the following randomly-generated landscape, where the target is **initially** at (41, 46).\n",
    "\n",
    "![Fixed Landscape Rule One v. Two Trials](./imgs/landscape_ruleOneTwoComparison.png)\n",
    "\n",
    "The comparison data generated is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RuleOne</th>\n",
       "      <td>4193</td>\n",
       "      <td>1535</td>\n",
       "      <td>1617</td>\n",
       "      <td>4949</td>\n",
       "      <td>1865</td>\n",
       "      <td>583</td>\n",
       "      <td>647</td>\n",
       "      <td>1549</td>\n",
       "      <td>3891</td>\n",
       "      <td>2623</td>\n",
       "      <td>...</td>\n",
       "      <td>4623</td>\n",
       "      <td>741</td>\n",
       "      <td>213</td>\n",
       "      <td>2001</td>\n",
       "      <td>3489</td>\n",
       "      <td>4207</td>\n",
       "      <td>2513</td>\n",
       "      <td>2145</td>\n",
       "      <td>2515</td>\n",
       "      <td>3265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleTwo</th>\n",
       "      <td>3825</td>\n",
       "      <td>297</td>\n",
       "      <td>1509</td>\n",
       "      <td>4045</td>\n",
       "      <td>3145</td>\n",
       "      <td>1173</td>\n",
       "      <td>121</td>\n",
       "      <td>6069</td>\n",
       "      <td>5247</td>\n",
       "      <td>545</td>\n",
       "      <td>...</td>\n",
       "      <td>17261</td>\n",
       "      <td>145</td>\n",
       "      <td>1059</td>\n",
       "      <td>25745</td>\n",
       "      <td>3599</td>\n",
       "      <td>2319</td>\n",
       "      <td>517</td>\n",
       "      <td>439</td>\n",
       "      <td>1759</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1     2     3     4     5    6     7     8     9    ...  \\\n",
       "RuleOne  4193  1535  1617  4949  1865   583  647  1549  3891  2623  ...   \n",
       "RuleTwo  3825   297  1509  4045  3145  1173  121  6069  5247   545  ...   \n",
       "\n",
       "           190  191   192    193   194   195   196   197   198   199  \n",
       "RuleOne   4623  741   213   2001  3489  4207  2513  2145  2515  3265  \n",
       "RuleTwo  17261  145  1059  25745  3599  2319   517   439  1759   687  \n",
       "\n",
       "[2 rows x 200 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fixed_ruleOneTwoComp_df = pd.read_csv('./data/fixed_map_comparison_ruleOneTwo.csv')[['RuleOne', 'RuleTwo']]\n",
    "fixed_ruleOneTwoComp_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above trial-by-trial data on the number of searches required for the agents (using rules 1 or 2) to find the target can be visualized via the following scatter plot:\n",
    "\n",
    "![Rules 1-2 Comparison by Trials, Scatter](./imgs/fixed_map_comparison_ruleOneTwoScatter.png)\n",
    "\n",
    "It appears that the agent using Rule 1 requires a higher number of searches to find the target. But this pattern is not immediately clear. To mediate this, we consider the quantity:\n",
    "\n",
    "$$\\text{Diff} = \\text{Number of searches}_{\\text{Rule1}} - \\text{Number of searches}_{\\text{Rule2}}$$\n",
    "\n",
    "![Rules 1-2 Difference by Trials, Plot](./imgs/fixed_map_comparison_ruleOneTwoDiff.png)\n",
    "\n",
    "![Rules 1-2 Difference by Trials, Box](./imgs/fixed_map_comparison_ruleOneTwoDiffBox.png)\n",
    "\n",
    "The above images show that the difference between the number of searches required following rules 1 and 2 has high spread. The following 1-variable statistics describe the distribution:\n",
    "\n",
    "- **Mean:** 426.55\n",
    "- **Standard Deviation:** 8418.86\n",
    "- **Min:** -49320\n",
    "- **Max:** 30422\n",
    "\n",
    "All in all, for a fixed map, it appears that **the agent using Rule 2** outperforms the agent using Rule 1 in terms of, on average, **427 fewer searches required to find the target.** However, the large variance visualized above strongly motivates repeated trials over multiple randomly-generated maps in order to see if this pattern holds in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated trials over multiple landscapes\n",
    "\n",
    "We conduct similar trials to compare both agents for $N = 50$ distinct and randomly generated maps. For each map and agent, we record the average number of actions taken to find the target over $n = 30$ trials. As above, we reset the target to a random new location in between each trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted agent movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule 3: Utility and decision-making strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison with unrestricted movement via rules 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A drunk man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A moving target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider a target which is not static in that:\n",
    "\n",
    "1. Upon every failed query, the target moves to one of its neighbors at random.\n",
    "2. Upon moving to one of its neighbors, some sensor returns an observation of terrain type (\"FLAT\" , \"HILLY\", etc.). However, the sensor is broken, so it returns some type that the new target is **not,** at random. \n",
    "\n",
    "For example, if we query a cell and do not find the target, and we get the new observation \"HILLY\", then we know that the target is now in a cell which is either \"FLAT\", \"FOREST\", or \"MAZE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Due to the object-oriented nature of our `Landscape` class, the target can easily be moved.\n",
    "\n",
    "The below code moves the target to one of its neighbors at random, and returns a terrain type the new target is not, at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveTarget(self):\n",
    "    x = self.target_x; y = self.target_y\n",
    "    self.landscape[x][y].target = ABSENT\n",
    "\n",
    "    nbrs = []\n",
    "    for dx, dy in dirs:\n",
    "        if 0 <= x + dx < self.dim and 0 <= y + dy < self.dim:\n",
    "            nbrs.append((x + dx, y + dy))\n",
    "\n",
    "    new_nbr = random.choice(nbrs)\n",
    "    self.target_x = new_nbr[0]; self.target_y = new_nbr[1]\n",
    "    self.landscape[self.target_x][self.target_y].target = PRESENT\n",
    "\n",
    "    terrains = {FLAT, HILLY, FOREST, MAZE}\n",
    "    new_terrain = {(self.landscape[self.target_x][self.target_y]).terrain}\n",
    "    return random.choice(list(terrains.difference(new_terrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A moving target AND an agent with restricted movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
